\section{Performance Assessment}
\label{sec:Performance Assessment}

We pursue a quantitative understanding of the performance of our method by considering a sphere of radius $R$ and center $\bm{C}$.  We assign a material identifier $m$ to each voxel in the image to define our spherical image mask; for the voxel with center at $\bm{v}$, the material assignment is:
\begin{linenomath}\begin{align} 
	m &=  \begin{cases}
		1, & \text{if}\ d \left(\bm{v},\bm{C}\right) \le R \\
		0, & \text{otherwise},
	\end{cases}
\end{align}\end{linenomath}
where $d(\cdot,\cdot)$ is the Euclidean distance. The voxel grid has dimensions $256 \times 256 \times 256$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Error Measures}
\label{Error Measures}

Two error metrics are defined: a {\em shape error} and a {\em volume error}. These metrics characterize the departure of the b-rep from an exact sphere. There are two sources of error~\cite{young_2008}: 1) error due to approximating the sphere with a binary image mask, or the {\em image-based error}; and 2) error due to reconstructing the binary image mask with a facetized b-rep. The latter dictates a method's ability to {\em converge to geometry}. The image mask defined above does not, of course, yield perfect image-based accuracy. In general, the object being scanned and segmented may not be well-defined or characterized, and as such, the image-based error is difficult to quantify. For this reason we select a sphere for quantitative comparison so that we have an analytical representation of what the true result should be. In this way we need not make assumptions about the image-based error, and can directly compare the facetized b-rep to the original intended object before it is even digitized into a raw image.

The shape error measures the surface-normal deviation of our algorithm's end-result b-rep from the exact sphere. The measure is constructed first by projecting each facet of the b-rep onto the exact sphere. The normal of each facet is compared to the normal of the exact sphere at the centroid of the projected facet by computing the magnitude of their cross product. An area-weighted sum is then taken over the b-rep, and finally the sum is normalized by the surface area of the b-rep. The error measure therefore lies in the range $\left [0,1\right]$. The shape error $e_s$ formula is:
\begin{equation} 
	e_s = \frac{ \sum \limits_{f\in\mathcal{F}} A_f \lVert {\bf n}_f \times {\bf n}_s \rVert}{\sum \limits_{f\in\mathcal{F}} A_f},
\end{equation}
where $A_f$ is the area and ${\bm n}_f$ is the unit normal of facet $f$, $\mathcal{F}$ is the set of all b-rep facets, and ${\bm n}_s$ is the normal of the exact sphere at the centroid of the projected facet onto the sphere.

The volume error measures the unsigned volume enclosed between the b-rep and the exact sphere. We calculate this measure via a sum of integrals over the projections $\gamma_f$ of each facet onto the exact sphere: 
\begin{equation}
\label{vol err}
	e_v = \frac{1}{4\pi R^2} \Big[\sum \limits_{f \in \mathcal{F}} \int \limits_{\gamma_f} (r - R)^2 d\alpha \Big]^{1/2} .
\end{equation}
Here, $r = \lVert {\bm x} \rVert$ is the distance from the sphere center to points on the planar facet associated with the projection $\gamma_f$, and $R$ is the radius of the exact sphere.  The indicated integrals are performed numerically on the projected facets. Six quadrature points were found to provide sufficient accuracy. The volume error is made dimensionless by normalizing by the surface area of the exact sphere.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Unpolluted Sphere}
\label{The Unpolluted Sphere}

The shape and volume errors were computed for the proposed method and for the three comparison approaches, for a perfect sphere embedded in a fixed $256 \times 256 \times 256$ array of voxels.  In the comparisons presented below, three independent parameters were varied:  1) b-rep resolution, 2) sphere radius, and 3) sphere-center location relative to the voxel grid. The purpose of these comparisons for clean input masks is to show that the proposed method performs comparably to other options, rather than specifically pointing to the superiority of one method over the other.

\figref{graph1} shows the shape and volume errors of the four methods as the b-rep resolution is varied for a spherical mask centered in the image with radius $R = 80$ voxels. See \figref{demos1} for a visual comparison of the image mask with representative b-reps from the proposed method and the other approaches. Default parameters were used in Simpleware for b-reps with $n \ge 2587$ vertices. For resolutions coarser than that, the ``target maximum error" was increased to allow results with $n < 2587$. When comparing the shape error of the different approaches, the proposed method performs competitively for coarse meshes ($n < 2500$), while it performs measurably better -- up to $40\%$ improvement -- for finer b-reps ($n > 5000$). Note that the shape error for marching cubes grows for higher b-rep resolutions due to aliasing -- when more points are used, the decimation step suppresses marching cubes' stair-step effect less strongly. Although the proposed method is subject to aliasing as well, this result suggests that the effect is significantly less than as is seen with marching cubes.  For the volume error, the proposed method performs up to $70\%$ better for b-reps with resolutions of $n < 5000$ vertices, and converges to a comparably small, but nonzero value as that of the other methods.  Note however that the volume errors are quite small for {\em all} methods tested.
\begin{figure}[b!]
\centering
\subfigure[]{%
	\includegraphics[scale=0.28]{media/7-performance/graph-resolution-shape.pdf}}
\subfigure[]{%
	\includegraphics[scale=0.28]{media/7-performance/graph-resolution-volume.pdf}}
%	
\caption{Comparison of (a) shape error and (b) volume error of the proposed method to those of the three comparison methods as the b-rep resolution is varied.}
\label{fig:graph1}
\end{figure}
\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.10]{media/7-performance/spheres-resolution.pdf}
	\caption{Comparison of proposed method with different approaches for selected b-rep resolutions.}
	\label{fig:demos1}
\end{figure}

\figref{graph2} shows the error metrics when the radius of the spherical mask is varied from 40 to 120 voxels, while keeping the sphere centered in the image and the b-rep resolution fixed at $n = 10428$ vertices. See \figref{demos2} for representative examples of the image mask and resulting b-reps for the different methods. The sphere radius represents an inverse measure of the curvature of the image mask relative to the voxel resolution; larger radii correspond to smoother surfaces. Both the shape and volume errors of the proposed method converge at least as quickly as the other methods, with the exception of the shape error for marching cubes, which has a noticeably larger error for smaller radii. The proposed method's shape error is the lowest of all methods tested over the entire range of sphere radii considered.  The volume error is very small for all methods over the range of sphere radii considered, but Simpleware and marching cubes exhibit some advantage over the proposed method and screened Poisson, particularly for small spheres.

\begin{figure}[b!]
	\centering
	\subfigure[]{%
		\includegraphics[scale=0.28]{media/7-performance/graph-size-shape.pdf}}
	\subfigure[]{%
		\includegraphics[scale=0.28]{media/7-performance/graph-size-volume.pdf}}
	%	
	\caption{Comparison of (a) shape error and (b) volume error of proposed method with different approaches as the sphere radius is varied.}
	\label{fig:graph2}
\end{figure}
\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.10]{media/7-performance/spheres-size.pdf}
	\caption{Comparison of proposed method with different approaches for selected sphere radii.}
	\label{fig:demos2}
\end{figure}

The methods are also compared for a sphere of $R = 80$ voxels that is translated along the direction $\bm{i}  + 2\bm{j} + 3\bm{k}$, while keeping the resolution fixed at $n = 10428$ vertices.  All methods show very low sensitivity to the translation of the object within the image, as shown in \figref{graph3}. The proposed method exhibits a relative standard deviation (RSD) that is smallest among the group in shape error ($1.1\%$), and highest in volume error ($2.1\%$). For reference, the RSDs of marching cubes, Simpleware, and screened Poisson surface reconstruction were $1.4\%$, $2.4\%$, and $1.9\%$ for shape error, and $1.3\%$, $1.7\%$, and $1.6\%$ for volume error.

\begin{figure}[b!]
	\centering
	\subfigure[]{%
		\includegraphics[scale=0.28]{media/7-performance/graph-center-shape.pdf}}
	\subfigure[]{%
		\includegraphics[scale=0.28]{media/7-performance/graph-center-volume.pdf}}
	%	
	\caption{Comparison of (a) shape error and (b) volume error for the four methods as the sphere center is translated.}
	\label{fig:graph3}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{{The Polluted Sphere}}
\label{The Polluted Sphere}

As a final point of quantitative comparison, the proposed method is compared to the other approaches as synthetic noise is introduced into the image mask. Specifically, voxel values are randomly flipped near the boundary of the sphere. This was done for a sphere with $R = 80$ voxels, and keeping the b-rep resolution as close to 10428 vertices as possible.  The introduction of noise created artifacts for some of the methods that triggered higher resolutions in unpredictable ways; for this reason we were unable to strictly control vertex count in the final b-reps. It was, however, verified that the b-rep resolution negligibly affected the results when studying the introduction of noise. \figref{graph4} shows shape and volume errors for the four methods as the percentage of randomly-flipped voxels near the boundary, or the {\em noise ratio}, is increased from $0\%$ to $20\%$. See \figref{demos4} for representative images.  For the proposed method, the shape and volume errors remain low, even for large values of the noise ratio. This tends to confirm the robustness of the proposed method in the presence of low-quality image-mask inputs.
\begin{figure}[t!]
	\centering
	\subfigure[]{%
		\includegraphics[scale=0.28]{media/7-performance/graph-noise-shape.pdf}}
	\subfigure[]{%
		\includegraphics[scale=0.28]{media/7-performance/graph-noise-volume.pdf}}
	%	
	\caption{Comparison of (a) shape error and (b) volume error of the four approaches as a function of noise ratio.}
	\label{fig:graph4}
\end{figure}
\begin{figure}[b!]
	\centering
	\includegraphics[scale=0.10]{media/7-performance/spheres-noise.pdf}
	\caption{{Comparison of proposed method with different approaches for selected mask noise.}}
	\label{fig:demos4}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Complex Surfaces}
\label{Complex Surfaces}

Lastly, we present several qualitative comparisons based on real image masks generated from MRI and CT.  All images were segmented using \textit{Seg3D} \cite{Seg3D}.  B-rep resolutions were matched in all cases, to make for fair comparisons.  In the case of the proposed method, each example completed on a 16 GB RAM laptop in less than 5 minutes. See \figref{example-meshes} for comparisons of the image masks and resulting b-reps.  Visual inspection suggests that the proposed method qualitatively performs at least as well as the comparison workflows, and visibly better over regions of low curvature.   In this connection, we mention that the proposed method readily accommodates an enhancement that admits sharp edges and vertices (or corners), making the method particularly suitable for industrial applications involving manufactured objects.  Work on this enhancement is in progress.  
\begin{figure}[h!]
	\centering
	 \includegraphics[scale=0.145]{media/7-performance/realExamples072022.pdf}
	\caption{{Comparison of proposed method with different approaches for various input image masks generated from MRI and CT scans. Note the proposed method's robustness to noisy inputs for the brain (second), distal femur (third), and equine proximal sesamoid (bottom). Sources: heart~\cite{cvgg}, brain~\cite{marcus_2007}, distal femur~\cite{epperson_2013}, pelvis~\cite{clark_2013}, L5 lumbar~\cite{yao_2016}, and equine proximal sesamoid~\cite{shaffer2021}}}
	\label{fig:example-meshes}
\end{figure}

Of note, too, is the proposed method's performance for noisy image masks, namely the brain, distal femur, and equine proximal sesamoid. The brain and distal femur masks were generated from MRI scans, and the proximal sesamoid from CT, reflecting the fact that noise may be present in either scanning modality.  For these cases, the b-reps produced by marching cubes and Simpleware exhibit artifacts due to the noise in the input image mask, while the screened-Poisson approach and the proposed method show more resilience to the noise for these datasets.  We note that specialized procedures and manual ``tweaking'' of the input image masks could have been used to optimize the image masks for each of the comparison methods in turn, thereby potentially improving the appearance of some of the final b-reps.  For example, the Simpleware software suite includes image-segmentation capabilities designed to work with the downstream image-processing functionality in Simpleware, such as use of ``partial-volume'' information.  However, in order to facilitate uniformity in the comparison, and in keeping with our stated focus on automation in the image-to-analysis pipeline, we chose to present the same segmented image to each of the four workflows.

